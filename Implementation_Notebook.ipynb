{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)*(1.0 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    \n",
    "    def __init__(self, architecture):\n",
    "        #architecture - numpy array with ith element representing the number of neurons in the ith layer.\n",
    "        \n",
    "        #Initialize the network architecture\n",
    "        self.L = architecture.size - 1 #The index of the last layer L\n",
    "        self.n = architecture #n stores the number of neurons in each layer\n",
    "        self.input_size = self.n[0] #input_size is the number of neurons in the first layer\n",
    "        self.output_size = self.n[self.L] #output_size is the number of neurons in the last layer\n",
    "        \n",
    "        #Parameters will store the network parameters, i.e. the weights and biases\n",
    "        self.parameters = {}\n",
    "        \n",
    "        #Initialize the network weights and biases:\n",
    "        for i in range (1, self.L + 1): \n",
    "            #Initialize weights to small random values\n",
    "            self.parameters['W' + str(i)] = np.random.randn(self.n[i], self.n[i - 1]) * 0.01\n",
    "            \n",
    "            #Initialize rest of the parameters to 1\n",
    "            self.parameters['b' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['z' + str(i)] = np.ones((self.n[i], 1))\n",
    "            self.parameters['a' + str(i)] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #As we started the loop from 1, we haven't initialized a[0]:\n",
    "        self.parameters['a0'] = np.ones((self.n[i], 1))\n",
    "        \n",
    "        #Initialize the cost:\n",
    "        self.parameters['C'] = 1\n",
    "        \n",
    "        #Create a dictionary for storing the derivatives:\n",
    "        self.derivatives = {}\n",
    "        \n",
    "        #Learning rate\n",
    "        self.alpha = 0.01\n",
    "            \n",
    "    def forward_propagate(self, X):\n",
    "        #Note that X here, is just one training example\n",
    "        self.parameters['a0'] = X\n",
    "        \n",
    "        #Calculate the activations for every layer l\n",
    "        for l in range(1, self.L + 1):\n",
    "            self.parameters['z' + str(l)] = np.add(np.dot(self.parameters['W' + str(l)], self.parameters['a' + str(l - 1)]), self.parameters['b' + str(l)])\n",
    "            self.parameters['a' + str(l)] = sigmoid(self.parameters['z' + str(l)])\n",
    "        \n",
    "    def compute_cost(self, y):\n",
    "        self.parameters['C'] = -(y*np.log(self.parameters['a' + str(self.L)]) + (1-y)*np.log( 1 - self.parameters['a' + str(self.L)]))\n",
    "    \n",
    "    def compute_derivatives(self, y):\n",
    "        #Partial derivatives of the cost function with respect to z[L], W[L] and b[L]:        \n",
    "        #dzL\n",
    "        self.derivatives['dz' + str(self.L)] = self.parameters['a' + str(self.L)] - y\n",
    "        #dWL\n",
    "        self.derivatives['dW' + str(self.L)] = np.dot(self.derivatives['dz' + str(self.L)], np.transpose(self.parameters['a' + str(self.L - 1)]))\n",
    "        #dbL\n",
    "        self.derivatives['db' + str(self.L)] = self.derivatives['dz' + str(self.L)]\n",
    "\n",
    "        #Partial derivatives of the cost function with respect to z[l], W[l] and b[l]\n",
    "        for l in range(self.L-1, 0, -1):\n",
    "            self.derivatives['dz' + str(l)] = np.dot(np.transpose(self.parameters['W' + str(l + 1)]), self.derivatives['dz' + str(l + 1)])*sigmoid_prime(self.parameters['z' + str(l)])\n",
    "            self.derivatives['dW' + str(l)] = np.dot(self.derivatives['dz' + str(l)], np.transpose(self.parameters['a' + str(l - 1)]))\n",
    "            self.derivatives['db' + str(l)] = self.derivatives['dz' + str(l)]\n",
    "            \n",
    "    def update_parameters(self):\n",
    "        for l in range(1, self.L+1):\n",
    "            self.parameters['W' + str(l)] -= self.alpha*self.derivatives['dW' + str(l)]\n",
    "            self.parameters['b' + str(l)] -= self.alpha*self.derivatives['db' + str(l)]\n",
    "        \n",
    "    def predict(self, x):\n",
    "        self.forward_propagate(x)\n",
    "        \n",
    "        #self.parameters['a0'] = X\n",
    "        \n",
    "        #Calculate the activations for every layer l\n",
    "        #for i in range(1, self.L + 1):\n",
    "         #   self.parameters['z' + str(i)] = np.add(np.dot(self.parameters['W' + str(i)], self.parameters['a' + str(i - 1)]), self.parameters['b' + str(i)])\n",
    "          #  self.parameters['a' + str(i)] = sigmoid(self.parameters['z' + str(i)])\n",
    "\n",
    "        return self.parameters['a' + str(self.L)]\n",
    "        \n",
    "    def fit(self, X, Y, num_iter):\n",
    "        for iter in range(0, num_iter):\n",
    "            c = 0\n",
    "            acc = 0\n",
    "            n_c = 0\n",
    "            for i in range(0, X.shape[0]):\n",
    "              x = X[i].reshape((X[i].size, 1))\n",
    "              y = Y[i]\n",
    "              self.forward_propagate(x)\n",
    "              self.compute_cost(y)\n",
    "              c += self.parameters['C'] \n",
    "              y_pred = self.predict(x)\n",
    "              y_pred = (y_pred > 0.5)\n",
    "              if y_pred == y:\n",
    "                  n_c += 1\n",
    "              self.compute_derivatives(y)\n",
    "              self.update_parameters()\n",
    "            \n",
    "            c = c/X.shape[0]\n",
    "            acc = (n_c/X.shape[0])*100\n",
    "            print('Iteration: ', iter)\n",
    "            print(\"Cost: \", c)\n",
    "            print(\"Accuracy:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('wheat-seeds-binary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_dataset = dataset.sample(frac=1).reset_index(drop=True)\n",
    "shuffled_dataset['Class'] = shuffled_dataset['Class'] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = shuffled_dataset.iloc[:, 0:-1].values\n",
    "y = shuffled_dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = np.array([7, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NeuralNetwork(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Cost:  [[0.73890065]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  1\n",
      "Cost:  [[0.70850564]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  2\n",
      "Cost:  [[0.69385541]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  3\n",
      "Cost:  [[0.68499343]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  4\n",
      "Cost:  [[0.67774778]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  5\n",
      "Cost:  [[0.67035692]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  6\n",
      "Cost:  [[0.66196914]]\n",
      "Accuracy: 55.10204081632652\n",
      "Iteration:  7\n",
      "Cost:  [[0.65207563]]\n",
      "Accuracy: 65.3061224489796\n",
      "Iteration:  8\n",
      "Cost:  [[0.64033605]]\n",
      "Accuracy: 76.53061224489795\n",
      "Iteration:  9\n",
      "Cost:  [[0.62656054]]\n",
      "Accuracy: 88.77551020408163\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 97.61904761904762\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "n_c = 0\n",
    "for i in range(0, X_test.shape[0]):\n",
    "  x = X_test[i].reshape((X_test[i].size, 1))\n",
    "  y = y_test[i]\n",
    "  y_pred = classifier.predict(x)\n",
    "  y_pred = (y_pred > 0.5)\n",
    "  #print('Expected: %d Got: %d' %(y, y_pred))\n",
    "  if y_pred == y:\n",
    "      n_c += 1\n",
    "\n",
    "acc = (n_c/X_test.shape[0])*100\n",
    "print(\"Test Accuracy\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
